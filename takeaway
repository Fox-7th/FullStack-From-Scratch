from curses import use_default_colors

from numpy import multiply


real -> complex; complex multiplication equals to rotate

Multiplication of Matrices:
dim 2 @ dim 2, as ususal
dim 3 @ dim 2 only last 2 dim matmul
dim 3 @ dim 3 only last 2 dim matmul
Summary: only check whether last 2 dims can matmul


映射：词表大小（token数量）维度 -> 嵌入维度
Embedding(x) 不是矩阵相乘Embed_mat @ x，而是 查表 方式，这个要注意
Embedding 的使用应该是 
1、Embedding矩阵的维度是[vocab_size, model_dim], 
2、输入的x 维度应该是[B, T]
转化为了 [B, T, model_dim]。
i.e.  B*T个key（范围0-vocab_size）每个key（比如说101），去除Embedding 矩阵的对应行（第101行）



普通的加减乘法不需要提高精度；
涉及 softmax、e^x 的复杂运算要提高精度”

工程上更精确的版本是：

✅ 线性运算（加减乘、矩阵乘、激活函数）→ 低精度即可
✅ 概率运算（exp、log、softmax、norm、loss）→ 必须 fp32



        # PyTorch中共享权重时只需要“参数对象相同”，不需要显式转置，因为在计算时维度会自动以不同方式被使用。
        # nn.Linear(in, out) 的 weight 形状是 [out, in]，而不是 [in, out]
        # nn.Linear(in, out) 的计算公式：y = x @ W.T + b






