Implemented a full-stack LLM training pipeline from scratch:
tokenizer → pretraining → SFT → DPO/RLHF → inference with KV cache & LoRA.
