Implemented a full-stack LLM training pipeline from scratch (MiniMind-scale):
tokenizer → pretraining → SFT → DPO/RLHF → inference with KV cache & LoRA.
